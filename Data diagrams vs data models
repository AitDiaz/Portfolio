PIPELINE BUILDING
1.- Define the use case: Continuing a previous example, our company ABC Inc. is in the asset management industry. They are migrating their storage to Snowflake's data lake. 
This time, they need a complete view of the data operations required to extract data from a given source, load it into Snowflake's data lake, and model the data to the curated layer (gold) using the medallion architecture.
For the data modeling task, we will use Ralph Kimball's comprehensible approach to data modeling

2.- Study requirements: After elicitation with the business partner, we can assert the following regarding the inputs and outputs for the pipeline:
                Inputs: CSV files. One of them very large. Both grow in a relatively predicted way, although the large one does so at a higher rate. 
                Outputs: The output of the pipeline is the bronze and silver layer of the data lake. The data needs to be cleansed, conformed in a semi-structured way. 
                Transformations: Transform and conform the data to meet the requirements of the bronze and silver architecture. 
                Non-functional requirements: reliability and robustness, the technologies that resonate with the team are Spark, Python, SQL. 

3.-Create a workflow: 
Data FLow Diagram

4.- Build a scalable architecture: The source files grow at a relatively predictable rate, so there is no massive need for scalability.
The larger file can be processed by Spark, which is known for its reliability on large amounts of data for batch processing jobs. 
The smaller file can be moved to Spark if needed at a later date. The business team sponsoring the change for the Python job is very familiar with object programming and uses Python regularly for many of its business processes. 

5.- Select technology:

Extracting Job:for the extran
Bigger File
Smaller file

Tranformation 
